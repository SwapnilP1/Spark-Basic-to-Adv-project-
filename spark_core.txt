# Execute below comment only in pyspark shell 

>>> from pyspark.sql.types import *
>>> from pyspark.sql import SparkSession
>>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType
>>> person_list = [("Berry","","Allen",1,"M"),
...         ("Oliver","Queen","",2,"M"),
...         ("Robert","","Williams",3,"M"),
...         ("Tony","","Stark",4,"F"),
...         ("Rajiv","Mary","Kumar",5,"F")
...     ]

# defining the schema 

schema = StructType([ \
...         StructField("firstname",StringType(),True), \
...         StructField("middlename",StringType(),True), \
...         StructField("middlename",StringType(),True), \
...         StructField("id", IntegerType(), True), \
...         StructField("gender", StringType(), True), \
...     ])

#creating spark dataframe

df = spark.createDataFrame(data=person_list,schema=schema)
#Printing data frame schema
#Printing data
df.show(truncate=False)
df.pySpark()

>>> df.printSchema()   -- print the schema 


 # Local file  data put in hdf 

 read data from hdfs if data is stored in any other folder 
>>> df1 = spark.read.option("header",True).csv("hdfs://namenode:8888/input_data/departments.csv")


# Read data from hdfs path 

we can see the data heading from these context 
#  df1 = spark.read.option("header",True).csv("/input_data/departments.csv")
>>> df1.show()

df1.printSchema()
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)


change string to interger value from interSchema command 

>>> df2 = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/departments.csv")
>>> df2.printSchema()
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)


another method for changing the inferschema foer any partycular column 

>>> df1 = spark.read.csv("/input_data/employees.csv",header = True, inferSchema = True)




-- # How to read the data and specifically mention any select statement

>>> emp_df = df1.select("EMPLOYEE_ID","FIRST_NAME").show()
--------------------------------------------------------
# First way of applying the select column 
>>> df1.select("EMPLOYEE_ID","FIRST_NAME").show()
--------------------------------------------------------
second way of selecting the data 
>>> df1.select(df1.EMPLOYEE_ID,df1.FIRST_NAME).show()
--------------------------------------------------------
third way to selecting any data 
>>> df1.select(df1["EMPLOYEE_ID"],df1["FIRST_NAME"].show()
---------------------------------------------------------

function is there for selecting sql query in pyspark 

>>> from pyspark.sql.functions import col


>>> df1.select(col("EMPLOYEE_ID"),col("FIRST_NAME")).show()

alisa method is aplicable with coloumn object 

>>> df1.select(col("EMPLOYEE_ID").alias("EMP_ID"),col("FIRST_NAME")).show()


>>> df1.select(col("EMPLOYEE_ID").alias("EMP_ID"),col("FIRST_NAME")).show()
+------+----------+
|EMP_ID|FIRST_NAME|
+------+----------+
|   198|    Donald|
|   199|   Douglas|
|   200|  Jennifer|
|   201|   Michael|
|   202|       Pat|
|   203|     Susan|
|   204|   Hermann|
|   205|   Shelley|
|   206|   William|
|   100|    Steven|
|   101|     Neena|
|   102|       Lex|
|   103| Alexander|
|   104|     Bruce|
|   105|     David|
|   106|     Valli|
|   107|     Diana|
|   108|     Nancy|
|   109|    Daniel|
|   110|      John|
+------+----------+

















