# Execute below comment only in pyspark shell 

>>> from pyspark.sql.types import *
>>> from pyspark.sql import SparkSession
>>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType
>>> person_list = [("Berry","","Allen",1,"M"),
...         ("Oliver","Queen","",2,"M"),
...         ("Robert","","Williams",3,"M"),
...         ("Tony","","Stark",4,"F"),
...         ("Rajiv","Mary","Kumar",5,"F")
...     ]

# defining the schema 

schema = StructType([ \
...         StructField("firstname",StringType(),True), \
...         StructField("middlename",StringType(),True), \
...         StructField("middlename",StringType(),True), \
...         StructField("id", IntegerType(), True), \
...         StructField("gender", StringType(), True), \
...     ])

#creating spark dataframe

df = spark.createDataFrame(data=person_list,schema=schema)
#Printing data frame schema
#Printing data
df.show(truncate=False)
df.pySpark()

>>> df.printSchema()   -- print the schema 


 # Local file  data put in hdf 

 read data from hdfs if data is stored in any other folder 
>>> df1 = spark.read.option("header",True).csv("hdfs://namenode:8888/input_data/departments.csv")


# Read data from hdfs path 

we can see the data heading from these context 
#  df1 = spark.read.option("header",True).csv("/input_data/departments.csv")
>>> df1.show()

df1.printSchema()
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)


change string to interger value from interSchema command 

>>> df2 = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/departments.csv")
>>> df2.printSchema()
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)


another method for changing the inferschema foer any partycular column 

>>> df1 = spark.read.csv("/input_data/employees.csv",header = True, inferSchema = True)

-- # How to read the data and specifically mention any select statement

>>> emp_df = df1.select("EMPLOYEE_ID","FIRST_NAME").show()
--------------------------------------------------------
# First way of applying the select column 
>>> df1.select("EMPLOYEE_ID","FIRST_NAME").show()
--------------------------------------------------------
second way of selecting the data 
>>> df1.select(df1.EMPLOYEE_ID,df1.FIRST_NAME).show()
--------------------------------------------------------
third way to selecting any data 
>>> df1.select(df1["EMPLOYEE_ID"],df1["FIRST_NAME"].show()
---------------------------------------------------------

function is there for selecting sql query in pyspark 

>>> from pyspark.sql.functions import col


>>> df1.select(col("EMPLOYEE_ID"),col("FIRST_NAME")).show()

alisa method is aplicable with coloumn object 

>>> df1.select(col("EMPLOYEE_ID").alias("EMP_ID"),col("FIRST_NAME")).show()


>>> df1.select(col("EMPLOYEE_ID").alias("EMP_ID"),col("FIRST_NAME")).show()
+------+----------+
|EMP_ID|FIRST_NAME|
+------+----------+
|   198|    Donald|
|   199|   Douglas|
|   200|  Jennifer|
|   201|   Michael|
|   202|       Pat|
|   203|     Susan|
|   204|   Hermann|
|   205|   Shelley|
|   206|   William|
|   100|    Steven|
|   101|     Neena|
|   102|       Lex|
|   103| Alexander|
|   104|     Bruce|
|   105|     David|
|   106|     Valli|
|   107|     Diana|
|   108|     Nancy|
|   109|    Daniel|
|   110|      John|
+------+----------+
creating a new column in pyspark and adding one more column 
-  add a column with increase salary to 1000 
>>> emp.select("EMPLOYEE_ID","FIRST_NAME","SALARY").withColumn("NEW_SALARY",col("SALARY") + 1000 ).show()
ANOTHER way to select the coloumn in spark 
>>> emp.withColumn("NEW_SALARY",col("SALARY") + 1000 ).select("EMPLOYEE_ID","FIRST_NAME","NEW_SALARY").show()
+-----------+----------+----------+
|EMPLOYEE_ID|FIRST_NAME|NEW_SALARY|
+-----------+----------+----------+
|        198|    Donald|      3600|
|        199|   Douglas|      3600|
|        200|  Jennifer|      5400|
|        201|   Michael|     14000|
|        202|       Pat|      7000|
|        203|     Susan|      7500|
|        204|   Hermann|     11000|
|        205|   Shelley|     13008|
|        206|   William|      9300|
|        100|    Steven|     25000|
|        101|     Neena|     18000|
|        102|       Lex|     18000|
|        103| Alexander|     10000|
|        104|     Bruce|      7000|
|        105|     David|      5800|
|        106|     Valli|      5800|
|        107|     Diana|      5200|
|        108|     Nancy|     13008|
|        109|    Daniel|     10000|
|        110|      John|      9200|
+-----------+----------+----------+
only showing top 20 rows
- same column  where we can add the new column data 
>>> emp.withColumn("SALARY",col("SALARY") - 1000 ).select("EMPLOYEE_ID","FIRST_NAME","SALARY").show()
+-----------+----------+------+
|EMPLOYEE_ID|FIRST_NAME|SALARY|
+-----------+----------+------+
|        198|    Donald|  1600|
|        199|   Douglas|  1600|
|        200|  Jennifer|  3400|
|        201|   Michael| 12000|
|        202|       Pat|  5000|
|        203|     Susan|  5500|
|        204|   Hermann|  9000|
|        205|   Shelley| 11008|
|        206|   William|  7300|
|        100|    Steven| 23000|
|        101|     Neena| 16000|
|        102|       Lex| 16000|
|        103| Alexander|  8000|
|        104|     Bruce|  5000|
|        105|     David|  3800|
|        106|     Valli|  3800|
|        107|     Diana|  3200|
|        108|     Nancy| 11008|
|        109|    Daniel|  8000|
|        110|      John|  7200|
+-----------+----------+------+

only showing the top 20 rows
# How to rename a column in pyspark
-  emp.withColumnRenamed("SALARY","EMP_SALARY").show()
# drop a particular column into the database 
>>> emp.drop("COMMISSION_PCT").show()
# filteraion in py spark 


 salary showing >  5000 
>>> emp1 = emp.filter(col("SALARY") > 5000)
>>> emp1 = emp.filter(col("SALARY") > 5000).select("FIRST_NAME","SALARY")
>>> emp1.show()
+-----------+------+
| FIRST_NAME|SALARY|
+-----------+------+
|    Michael| 13000|
|        Pat|  6000|
|      Susan|  6500|
|    Hermann| 10000|
|    Shelley| 12008|
|    William|  8300|
|     Steven| 24000|
|      Neena| 17000|
|        Lex| 17000|
|  Alexander|  9000|
|      Bruce|  6000|
|      Nancy| 12008|
|     Daniel|  9000|
|       John|  8200|
|     Ismael|  7700|
|Jose Manuel|  7800|
|       Luis|  6900|
|        Den| 11000|
|    Matthew|  8000|
|       Adam|  8200|
+-----------+------+
only showing the top 20 rows

#  filter condition and where the condition 
>>> emp.filter((col("DEPARTMENT_ID") == 50) & (col("SALARY") < 5000 )
).select ("FIRST_NAME","SALARY","DEPARTMENT_ID").show()

>>> emp.distinct().show()
# drop duplicated data in the query 
>>> emp.dropDuplicates().show()

# In these filter conditions we have to identify the department id which is not equal to 50 

>>> emp.filter("DEPARTMENT_ID  <> 50 ").select ("FIRST_NAME","SALARY","DEPARTMENT_ID").show()


Reading data what kind of cleaning do you apply? 

# basic things 

1 removing null value 
2 remove repetitive data 
   
 
# drop the duplicate data 
>>> emp.drop duplicates(["DEPARTMENT_ID","HIRE_DATE"]).show(100)


# import all SQL function in the list ------

 -  from pyspark.sql.functions import * 

count the salary column 
>>> emp.select(count("SALARY")).show(0)

# count with alias name 
 emp.select(count("SALARY")).alias("SALARY_COUNT").show()

+------------+
|SALARY_COUNT|
+------------+
|          50|
+------------+

# max salary ( show) 

>>> emp.select(max("SALARY").alias("MAX_SALARY")).show()
+----------+
|MAX_SALARY|
+----------+
|     24000|
+----------+














































































































