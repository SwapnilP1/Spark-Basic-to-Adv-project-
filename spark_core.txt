# Execute below comment only in pyspark shell 

>>> from pyspark.sql.types import *
>>> from pyspark.sql import SparkSession
>>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType
>>> person_list = [("Berry","","Allen",1,"M"),
...         ("Oliver","Queen","",2,"M"),
...         ("Robert","","Williams",3,"M"),
...         ("Tony","","Stark",4,"F"),
...         ("Rajiv","Mary","Kumar",5,"F")
...     ]

# defining the schema 

schema = StructType([ \
...         StructField("firstname",StringType(),True), \
...         StructField("middlename",StringType(),True), \
...         StructField("middlename",StringType(),True), \
...         StructField("id", IntegerType(), True), \
...         StructField("gender", StringType(), True), \
...     ])

#creating spark dataframe

df = spark.createDataFrame(data=person_list,schema=schema)
#Printing data frame schema
#Printing data
df.show(truncate=False)
df.pySpark()

>>> df.printSchema()   -- print the schema 


 # Local file  data put in hdf 

 read data from hdfs if data is stored in any other folder 
>>> df1 = spark.read.option("header",True).csv("hdfs://namenode:8888/input_data/departments.csv")


# Read data from hdfs path 

we can see the data heading from these context 
#  df1 = spark.read.option("header",True).csv("/input_data/departments.csv")
>>> df1.show()

df1.printSchema()
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)


change string to interger value from interSchema command 

>>> df2 = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/departments.csv")
>>> df2.printSchema()
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)


another method for changing the inferschema foer any partycular column 

>>> df1 = spark.read.csv("/input_data/employees.csv",header = True, inferSchema = True)

-- # How to read the data and specifically mention any select statement

>>> emp_df = df1.select("EMPLOYEE_ID","FIRST_NAME").show()
--------------------------------------------------------
# First way of applying the select column 
>>> df1.select("EMPLOYEE_ID","FIRST_NAME").show()
--------------------------------------------------------
second way of selecting the data 
>>> df1.select(df1.EMPLOYEE_ID,df1.FIRST_NAME).show()
--------------------------------------------------------
third way to selecting any data 
>>> df1.select(df1["EMPLOYEE_ID"],df1["FIRST_NAME"].show()
---------------------------------------------------------

function is there for selecting sql query in pyspark 

>>> from pyspark.sql.functions import col


>>> df1.select(col("EMPLOYEE_ID"),col("FIRST_NAME")).show()

alisa method is aplicable with coloumn object 

>>> df1.select(col("EMPLOYEE_ID").alias("EMP_ID"),col("FIRST_NAME")).show()


>>> df1.select(col("EMPLOYEE_ID").alias("EMP_ID"),col("FIRST_NAME")).show()
+------+----------+
|EMP_ID|FIRST_NAME|
+------+----------+
|   198|    Donald|
|   199|   Douglas|
|   200|  Jennifer|
|   201|   Michael|
|   202|       Pat|
|   203|     Susan|
|   204|   Hermann|
|   205|   Shelley|
|   206|   William|
|   100|    Steven|
|   101|     Neena|
|   102|       Lex|
|   103| Alexander|
|   104|     Bruce|
|   105|     David|
|   106|     Valli|
|   107|     Diana|
|   108|     Nancy|
|   109|    Daniel|
|   110|      John|
+------+----------+
creating a new column in pyspark and adding one more column 
-  add a column with increase salary to 1000 
>>> emp.select("EMPLOYEE_ID","FIRST_NAME","SALARY").withColumn("NEW_SALARY",col("SALARY") + 1000 ).show()
ANOTHER way to select the coloumn in spark 
>>> emp.withColumn("NEW_SALARY",col("SALARY") + 1000 ).select("EMPLOYEE_ID","FIRST_NAME","NEW_SALARY").show()
+-----------+----------+----------+
|EMPLOYEE_ID|FIRST_NAME|NEW_SALARY|
+-----------+----------+----------+
|        198|    Donald|      3600|
|        199|   Douglas|      3600|
|        200|  Jennifer|      5400|
|        201|   Michael|     14000|
|        202|       Pat|      7000|
|        203|     Susan|      7500|
|        204|   Hermann|     11000|
|        205|   Shelley|     13008|
|        206|   William|      9300|
|        100|    Steven|     25000|
|        101|     Neena|     18000|
|        102|       Lex|     18000|
|        103| Alexander|     10000|
|        104|     Bruce|      7000|
|        105|     David|      5800|
|        106|     Valli|      5800|
|        107|     Diana|      5200|
|        108|     Nancy|     13008|
|        109|    Daniel|     10000|
|        110|      John|      9200|
+-----------+----------+----------+
only showing top 20 rows
- same column  where we can add the new column data 
>>> emp.withColumn("SALARY",col("SALARY") - 1000 ).select("EMPLOYEE_ID","FIRST_NAME","SALARY").show()
+-----------+----------+------+
|EMPLOYEE_ID|FIRST_NAME|SALARY|
+-----------+----------+------+
|        198|    Donald|  1600|
|        199|   Douglas|  1600|
|        200|  Jennifer|  3400|
|        201|   Michael| 12000|
|        202|       Pat|  5000|
|        203|     Susan|  5500|
|        204|   Hermann|  9000|
|        205|   Shelley| 11008|
|        206|   William|  7300|
|        100|    Steven| 23000|
|        101|     Neena| 16000|
|        102|       Lex| 16000|
|        103| Alexander|  8000|
|        104|     Bruce|  5000|
|        105|     David|  3800|
|        106|     Valli|  3800|
|        107|     Diana|  3200|
|        108|     Nancy| 11008|
|        109|    Daniel|  8000|
|        110|      John|  7200|
+-----------+----------+------+

only showing the top 20 rows
# How to rename a column in pyspark
-  emp.withColumnRenamed("SALARY","EMP_SALARY").show()
# drop a particular column into the database 
>>> emp.drop("COMMISSION_PCT").show()
# filteraion in py spark 


 salary showing >  5000 
>>> emp1 = emp.filter(col("SALARY") > 5000)
>>> emp1 = emp.filter(col("SALARY") > 5000).select("FIRST_NAME","SALARY")
>>> emp1.show()
+-----------+------+
| FIRST_NAME|SALARY|
+-----------+------+
|    Michael| 13000|
|        Pat|  6000|
|      Susan|  6500|
|    Hermann| 10000|
|    Shelley| 12008|
|    William|  8300|
|     Steven| 24000|
|      Neena| 17000|
|        Lex| 17000|
|  Alexander|  9000|
|      Bruce|  6000|
|      Nancy| 12008|
|     Daniel|  9000|
|       John|  8200|
|     Ismael|  7700|
|Jose Manuel|  7800|
|       Luis|  6900|
|        Den| 11000|
|    Matthew|  8000|
|       Adam|  8200|
+-----------+------+
only showing the top 20 rows

#  filter condition and where the condition 
>>> emp.filter((col("DEPARTMENT_ID") == 50) & (col("SALARY") < 5000 )
).select ("FIRST_NAME","SALARY","DEPARTMENT_ID").show()

>>> emp.distinct().show()
# drop duplicated data in the query 
>>> emp.dropDuplicates().show()

# In these filter conditions we have to identify the department id which is not equal to 50 

>>> emp.filter("DEPARTMENT_ID  <> 50 ").select ("FIRST_NAME","SALARY","DEPARTMENT_ID").show()


Reading data what kind of cleaning do you apply? 

# basic things 

1 removing null value 
2 remove repetitive data 
   
 
# drop the duplicate data 
>>> emp.drop duplicates(["DEPARTMENT_ID","HIRE_DATE"]).show(100)

# import all SQL functions in the list ------
 -  from pyspark.sql.functions import * 

count the salary column 
>>> emp.select(count("SALARY")).show(0)

# count with an alias name 
 emp.select(count("SALARY")).alias("SALARY_COUNT").show()

+------------+
|SALARY_COUNT|
+------------+
|          50|
+------------+

# max salary ( show) 
>>> emp.select(max("SALARY").alias("MAX_SALARY")).show()
+----------+
|MAX_SALARY|
+----------+
|     24000|
+----------+

# average salary - average salary 
>>> emp.select(avg("SALARY").alias("averagae_SALARY")).show()
+---------------+
|averagae_SALARY|
+---------------+
|        6182.32|
+---------------+

# ascending order department data 
>>> emp.select("EMPLOYEE_ID","FIRST_NAME","DEPARTMENT_ID","SALARY").orderBy(col("DEPARTMENT_ID").asc()).show()

# group by command in pyspark 

calculate the department-wise   total salary 

>>> emp.groupby("DEPARTMENT_ID").sum("SALARY").show() 

pulling 2 columns 

>>> emp.groupby("DEPARTMENT_ID","JOB_ID").sum("SALARY").show(100)        



>>> emp.groupBy("DEPARTMENT_ID","JOB_ID").sum("SALARY","EMPLOYEE_ID").show(100)


+-------------+----------+-----------+----------------+

|DEPARTMENT_ID|    JOB_ID|sum(SALARY)|sum(EMPLOYEE_ID)|
+-------------+----------+-----------+----------------+
|           90|   AD_PRES|      24000|             100|
|           30|    PU_MAN|      11000|             114|
|           70|    PR_REP|      10000|             204|
|           50|    ST_MAN|      36400|             610|
|           40|    HR_REP|       6500|             203|
|           60|   IT_PROG|      28800|             525|
|           10|   AD_ASST|       4400|             200|
|           30|  PU_CLERK|      13900|             585|
|           50|  ST_CLERK|      44000|            2120|
|           20|    MK_REP|       6000|             202|
|           50|  SH_CLERK|       5200|             397|
|           90|     AD_VP|      34000|             203|
|          100|FI_ACCOUNT|      39600|             555|
|          110|    AC_MGR|      12008|             205|
|          110|AC_ACCOUNT|       8300|             206|
|           20|    MK_MAN|      13000|             201|
|          100|    FI_MGR|      12008|             108|
+-------------+----------+-----------+----------------+


# group by pyspark command  

>>> emp.groupBy("DEPARTMENT_ID").agg(sum("SALARY").alias("Sum_Salary"),max("SALARY").alias("MAX_SALARY"),min("SALARY").alias("MIN_SALARY"),avg("SALARY").alias("AVG_SALARY")).show()


+-------------+----------+----------+----------+------------------+
|DEPARTMENT_ID|Sum_Salary|MAX_SALARY|MIN_SALARY|        AVG_SALARY|
+-------------+----------+----------+----------+------------------+
|           20|     19000|     13000|      6000|            9500.0|
|           40|      6500|      6500|      6500|            6500.0|
|          100|     51608|     12008|      6900| 8601.333333333334|
|           10|      4400|      4400|      4400|            4400.0|
|           50|     85600|      8200|      2100|3721.7391304347825|
|           70|     10000|     10000|     10000|           10000.0|
|           90|     58000|     24000|     17000|19333.333333333332|
|           60|     28800|      9000|      4200|            5760.0|
|          110|     20308|     12008|      8300|           10154.0|
|           30|     24900|     11000|      2500|            4150.0|
+-------------+----------+----------+----------+------------------+

# group by and where the command 
-----------------------------------

------------------------------------------------------------------------
>> emp.groupBy("DEPARTMENT_ID").agg(sum("SALARY").alias("Sum_Salary"),max("SALARY").alias("MAX_SALARY"),min("SALARY").alias("MIN_SALARY"),avg("SALARY").alias("AVG_SALARY")).where(col("MAX_SALARY") > 10000).show()

+-------------+----------+----------+----------+------------------+
|DEPARTMENT_ID|Sum_Salary|MAX_SALARY|MIN_SALARY|        AVG_SALARY|
+-------------+----------+----------+----------+------------------+
|           20|     19000|     13000|      6000|            9500.0|
|          100|     51608|     12008|      6900| 8601.333333333334|
|           90|     58000|     24000|     17000|19333.333333333332|
|          110|     20308|     12008|      8300|           10154.0|
|           30|     24900|     11000|      2500|            4150.0|
+-------------+----------+----------+----------+------------------+

--------------------------------------------------------------------------------------
the case when a statement  in SQL 

>>> df = emp.withColumn("EMP_GRADE",when( col("SALARY") > 15000, "A").when((col("SALARY") >= 10000 ) & ( col ("SALARY") < 15000),"B").otherwise("c")).show(10)
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+
|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|EMP_GRADE|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+
|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|        c|
|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|        c|
|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|        c|
|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|        B|
|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|        c|
|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|        c|
|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|        B|
|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|        B|
|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|        c|
|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|        A|
+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+---------+





>>> df = emp.withColumn("EMP_GRADE",when( col("SALARY") > 15000, "A").when((col("SALARY") >= 10000 ) & ( col ("SALARY") < 15000),"B").otherwise("c
"))
>>> df.select("EMPLOYEE_ID","SALARY","EMP_GRADE").show()
+-----------+------+---------+
|EMPLOYEE_ID|SALARY|EMP_GRADE|
+-----------+------+---------+
|        198|  2600|        c|
|        199|  2600|        c|
|        200|  4400|        c|
|        201| 13000|        B|
|        202|  6000|        c|
|        203|  6500|        c|
|        204| 10000|        B|
|        205| 12008|        B|
|        206|  8300|        c|
|        100| 24000|        A|
|        101| 17000|        A|
|        102| 17000|        A|
|        103|  9000|        c|
|        104|  6000|        c|
|        105|  4800|        c|
|        106|  4800|        c|
|        107|  4200|        c|
|        108| 12008|        B|
|        109|  9000|        c|
|        110|  8200|        c|
+-----------+------+---------+>>> df = emp.withColumn("EMP_GRADE",when( col("SALARY") > 15000, "A").when((col("SALARY") >= 10000 ) & ( col ("SALARY") < 15000),"B").otherwise("c
"))
>>> df.select("EMPLOYEE_ID","SALARY","EMP_GRADE").show()
+-----------+------+---------+
|EMPLOYEE_ID|SALARY|EMP_GRADE|
+-----------+------+---------+
|        198|  2600|        c|
|        199|  2600|        c|
|        200|  4400|        c|
|        201| 13000|        B|
|        202|  6000|        c|
|        203|  6500|        c|
|        204| 10000|        B|
|        205| 12008|        B|
|        206|  8300|        c|
|        100| 24000|        A|
|        101| 17000|        A|
|        102| 17000|        A|
|        103|  9000|        c|
|        104|  6000|        c|
|        105|  4800|        c|
|        106|  4800|        c|
|        107|  4200|        c|
|        108| 12008|        B|
|        109|  9000|        c|
|        110|  8200|        c|
+-----------+------+---------+

----- # To convert in SQL data queries 

emp.createOrReplaceTempView("employee")

>>> df.show()
+-----------+
|employee_id|
+-----------+
|        198|
|        199|
|        200|
|        201|
|        202|
|        203|
|        204|
|        205|



>>> df = spark.sql("select department_id, sum(salary)  as salary  from employee group by department_id ")
>>> df.show()
+-------------+------+
|department_id|salary|
+-------------+------+
|           20| 19000|
|           40|  6500|
|          100| 51608|
|           10|  4400|
|           50| 85600|
|           70| 10000|
|           90| 58000|
|           60| 28800|
|          110| 20308|
|           30| 24900|
+-------------+------+

 # rank function in spark 

>>> spark.sql("select employee_id,department_id, rank()over (partition by department_id  order by salary desc) as rank_salary from employee ").show()
+-----------+-------------+-----------+
|employee_id|department_id|rank_salary|
+-----------+-------------+-----------+
|        200|           10|          1|


|        201|           20|          1|
|        202|           20|          2|
|        114|           30|          1|
|        115|           30|          2|
|        116|           30|          3|
|        117|           30|          4|
|        118|           30|          5|
|        119|           30|          6|
|        203|           40|          1|
|        121|           50|          1|
|        120|           50|          2|
|        122|           50|          3|
|        123|           50|          4|
|        124|           50|          5|
|        137|           50|          6|
|        129|           50|          7|
|        133|           50|          7|
|        125|           50|          9|
|        138|           50|          9|
+-----------+-------------+-----------+
only showing the top 20 rows


# join in dept side 

>>> emp.join(dep,emp.DEPARTMENT_ID  == dep.DEPARTMENT_ID, "inner").select(emp.EMPLOYEE_ID,emp.DEPARTMENT_ID,dep.DEPARTMENT_NAME).show()
-------------------------------------------
|EMPLOYEE_ID|DEPARTMENT_ID|DEPARTMENT_NAME|
+-----------+-------------+---------------+
|        200|           10| Administration|
|        202|           20|      Marketing|
|        201|           20|      Marketing|
|        119|           30|     Purchasing|
|        118|           30|     Purchasing|
|        117|           30|     Purchasing|
|        116|           30|     Purchasing|
|        115|           30|     Purchasing|
|        114|           30|     Purchasing|
|        203|           40|Human Resources|
|        140|           50|       Shipping|
|        139|           50|       Shipping|
|        138|           50|       Shipping|
|        137|           50|       Shipping|
|        136|           50|       Shipping|
|        135|           50|       Shipping|
|        134|           50|       Shipping|
|        133|           50|       Shipping|
|        132|           50|       Shipping|
|        131|           50|       Shipping|
+-----------+-------------+---------------+



































































































